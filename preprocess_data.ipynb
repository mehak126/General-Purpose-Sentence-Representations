{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from collections import Counter\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_file = '../data/shuf_allnli.txt'\n",
    "out_file = '../data/nli_data_tokenized.txt'\n",
    "nli_dev_file = '../data/nli_dev.txt'\n",
    "out_dev_file = '../data/nli_dev_tokenized.txt'\n",
    "\n",
    "cp_in_file = '../data/en.txt'\n",
    "cp_lab_file = '../data/pt.txt'\n",
    "cp_out_file = '../data/tree.txt'\n",
    "\n",
    "vocab_file = '../data/vocab.txt'\n",
    "cp_vocab_file = '../data/cp_label.txt'\n",
    "cp_temp_file1 = '../data/cp_temp1.txt'\n",
    "cp_temp_file2 = '../data/cp_temp2.txt'\n",
    "\n",
    "nmt_english = '../data/english.txt'\n",
    "nmt_german = '../data/german.txt'\n",
    "nmt_temp1 = '../data/preprocessed_english.txt'\n",
    "nmt_temp2 = '../data/preprocessed_german.txt'\n",
    "nmt_out = '../data/nmt_data.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(input_files, vocab_size): #pass a list of input files (already tokenized) to create the vocab on\n",
    "    freq = Counter()\n",
    "    for file in input_files:\n",
    "        with open(file,'r') as fr:\n",
    "            for index, line in enumerate(fr):\n",
    "#                 if index == 10:\n",
    "#                     break\n",
    "\n",
    "#                 line = re.sub('([.,!?()#$%&/:;<=>?@{|}~\\'\\\"])'  , r' \\1 ', line)\n",
    "    \n",
    "                tokenized = line.strip().split()\n",
    "#                 sen = line.strip().split('\\t')\n",
    "                for word in tokenized:\n",
    "                    freq[word] += 1\n",
    "                if index%100000 == 0:\n",
    "                    print(index)\n",
    "                    \n",
    "    most_common_words = freq.most_common(vocab_size)\n",
    "    print(\"total words: \" + str(len(freq)))\n",
    "    \n",
    "    vocab = Counter()\n",
    "    vocab['UNK'] = 0\n",
    "    vocab['PAD'] = 1\n",
    "    vocab['EOS'] = 2\n",
    "    \n",
    "    for i in range(vocab_size-3):\n",
    "        vocab[most_common_words[i][0]] = i+2\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vocab_file(vocab_file, vocab):\n",
    "    with open(vocab_file,'w') as fw:\n",
    "        for word in vocab.keys():\n",
    "            fw.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLI_preprocessing(nli_file, out_file):\n",
    "    # with open(nli_file,'r') as fr, open(out1,'w') as fw1, open(out2,'w') as fw2, open(out3,'w') as fw3:\n",
    "    with open(nli_file,'r') as fr, open(out_file,'w') as fw:\n",
    "        for index, line in enumerate(fr):\n",
    "            if index == 0:\n",
    "                continue\n",
    "            components = line.split('\\t')\n",
    "            sen1 = components[5].strip().lower()\n",
    "            trantab = sen1.maketrans({key: None for key in string.punctuation})\n",
    "            sen1 = word_tokenize(sen1.translate(trantab))\n",
    "\n",
    "            sen2 = components[6].strip().lower()\n",
    "            trantab = sen2.maketrans({key: None for key in string.punctuation})\n",
    "            sen2 = word_tokenize(sen2.translate(trantab))\n",
    "\n",
    "            label = components[0].strip().lower()\n",
    "            fw.write(' '.join(sen1) + '\\t' + ' '.join(sen2) + '\\t' + label + '\\n')\n",
    "            if index % 100000 == 0:\n",
    "                print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CP_preprocessing(sen_file, out_file):\n",
    "        with open(sen_file, 'r') as fr1, open(out_file, 'w') as fw:\n",
    "            count = 0\n",
    "            for line1 in fr1:\n",
    "                count += 1\n",
    "#                 line2 = re.sub('([.,!?()#$%&/:;<=>?@{|}~\\'\\\"])'  , r' \\1 ', line2)\n",
    "                line1 = re.sub('([.,!?()#$%&/:;<=>?@{|}~\\'\\\"])'  , r' \\1 ', line1)\n",
    "                fw.write(line1)\n",
    "                if count % 10000 == 0:\n",
    "                    print(count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMT_preprocessing(sen_file, out_file):\n",
    "    with open(sen_file, 'r') as fr, open(out_file,'w') as fw:\n",
    "        for index, line in enumerate(fr):\n",
    "            line = line.strip().lower()\n",
    "            trantab = line.maketrans({key: None for key in string.punctuation})\n",
    "            sen1 = word_tokenize(line.translate(trantab))\n",
    "            fw.write(' '.join(sen1) + '\\n')\n",
    "            if index%100000 == 0:\n",
    "                print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_files(f1, f2, of):\n",
    "    with open(f1, 'r') as fr1, open(f2, 'r') as fr2, open(of,'w') as fw:\n",
    "        count = 0\n",
    "        for line1, line2 in (zip(fr1,fr2)):\n",
    "            count += 1\n",
    "            if line1.rstrip().strip() == '' or line2.rstrip().strip() == '':\n",
    "                continue\n",
    "            fw.write(line1.strip() + '\\t' + line2.strip() + '\\n')\n",
    "            if count%10000 == 0:\n",
    "                print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLI_preprocessing(nli_file, out_file)\n",
    "NLI_preprocessing(nli_dev_file, out_dev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP_preprocessing(cp_in_file, cp_temp_file1)\n",
    "CP_preprocessing(cp_lab_file, cp_temp_file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_files(cp_temp_file1, cp_temp_file2, cp_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMT_preprocessing(nmt_english, nmt_temp1)\n",
    "NMT_preprocessing(nmt_german, nmt_temp2)\n",
    "join_files(nmt_temp1, nmt_temp2, nmt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = create_vocab([nli_fi?le1, nli_file2, nli_file3], vocab_size) #list of input files, size of vocab\n",
    "vocab = create_vocab([out_file, cp_out_file, nmt_temp1], vocab_size) #list of input files, size of vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_files(nmt_temp1, nmt_temp2, nmt_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vocab_file(vocab_file,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_nmt = create_vocab([nmt_temp2], vocab_size)\n",
    "write_vocab_file('../data/german_vocab.txt', vocab_nmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_cp = create_vocab([cp_temp_file2], vocab_size) #list of input files, size of vocab\n",
    "write_vocab_file(cp_vocab_file,vocab_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
