{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tf.enable_eager_execution()\n",
    "from tensorflow.python.ops import lookup_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nli_file = '../data/nli_data_tokenized.txt'\n",
    "# nli_file = '../data/small_dataset.txt'\n",
    "vocab_file = '../data/vocab.txt'\n",
    "label_vocab_file = '../data/label_vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all hyperparameters\n",
    "vocab_size = 30000\n",
    "embedding_size = 256\n",
    "batch_size = 256\n",
    "num_neurons = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging = tf.logging\n",
    "logging.set_verbosity(logging.INFO)\n",
    "\n",
    "def log_msg(msg):\n",
    "   logging.info(f'{time.ctime()}: {msg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLI_create_dataset(nli_file, vocab_table, label_vocab ,batch_size):\n",
    "    dataset = tf.data.TextLineDataset(nli_file)\n",
    "    dataset = dataset.map(lambda sentence: (tf.string_split([sentence],'\\t').values[0], tf.string_split([sentence],'\\t').values[1], tf.string_split([sentence],'\\t').values[2]))\n",
    "    dataset = dataset.map(lambda s1, s2, lab : (tf.string_split([s1]).values, tf.string_split([s2]).values, lab ))\n",
    "    dataset = dataset.map(lambda s1, s2, lab: (s1, tf.size(s1), s2, tf.size(s2), lab  ))\n",
    "    dataset = dataset.map(lambda s1, l1, s2, l2, lab: (vocab_table.lookup(s1), l1, vocab_table.lookup(s2), l2, label_vocab.lookup(lab)   ) )\n",
    "    dataset = dataset.padded_batch(batch_size=batch_size ,padded_shapes=([None], [],[None],[], []))\n",
    "    return dataset    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Embedding model\n",
    "# class Embedding(tf.keras.Model):\n",
    "#     def __init__(self, V, d):\n",
    "#         super(Embedding, self).__init__()\n",
    "#         self.W = tfe.Variable(tf.random_uniform(minval=-1.0, maxval=1.0, shape=[V, d]))\n",
    "    \n",
    "#     def call(self, word_indexes):\n",
    "#         return tf.nn.embedding_lookup(self.W, word_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(tf.keras.Model):\n",
    "    def __init__(self, V, d, init):\n",
    "        super(Embedding, self).__init__()\n",
    "#         self.W = tfe.Variable(tf.random_uniform(minval=-1.0, maxval=1.0, shape=[V, d]))\n",
    "        self.W = tfe.Variable(init)\n",
    "    \n",
    "    def call(self, word_indexes):\n",
    "        return tf.nn.embedding_lookup(self.W, word_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StaticRNN(tf.keras.Model):\n",
    "    def __init__(self, h, cell):\n",
    "        super(StaticRNN, self).__init__()\n",
    "        if cell == 'lstm':\n",
    "            self.cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=h)\n",
    "        elif cell == 'gru':\n",
    "            self.cell = tf.nn.rnn_cell.GRUCell(num_units=h)\n",
    "        else:\n",
    "            self.cell = tf.nn.rnn_cell.BasicRNNCell(num_units=h)\n",
    "        \n",
    "        \n",
    "    def call(self, word_vectors, num_words, state, init_state):\n",
    "        word_vectors_time = tf.unstack(word_vectors, axis=1)\n",
    "        if state:\n",
    "            outputs, final_state = tf.nn.static_rnn(cell=self.cell, initial_state = init_state,  sequence_length=num_words, inputs=word_vectors_time, dtype=tf.float32)\n",
    "        else:\n",
    "            outputs, final_state = tf.nn.static_rnn(cell=self.cell,  sequence_length=num_words, inputs=word_vectors_time, dtype=tf.float32)\n",
    "        return outputs, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, V, d, h, cell):\n",
    "        super(Encoder, self).__init__()\n",
    "        init = tf.random_uniform(minval=-1.0, maxval=1.0, shape=[V, d])\n",
    "        self.word_embedding = Embedding(V, d, init)\n",
    "        self.rnn = StaticRNN(h, cell)\n",
    "\n",
    "        \n",
    "    def call(self, datum, lens, state, init_state):\n",
    "        word_vectors = self.word_embedding(datum)        \n",
    "        logits, final_state = self.rnn(word_vectors, lens, state, init_state)\n",
    "        batch_outputs = []\n",
    "        for i in range(int(tf.size(lens))):\n",
    "            sen_len = int(lens[i])\n",
    "            batch_outputs.append(logits[sen_len-1][i])\n",
    "\n",
    "#         return logits[-1], final_state\n",
    "        return tf.convert_to_tensor(batch_outputs), final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLI_Decoder(tf.keras.Model):\n",
    "    def __init__(self, h):\n",
    "        super(NLI_Decoder, self).__init__()\n",
    "        self.mlp = tf.keras.layers.Dense(units=h)\n",
    "        self.output_layer = tf.keras.layers.Dense(units=3) #3 output categories\n",
    "\n",
    "    def call(self, datum, dropout): #datum is actually concatenated output from rnn\n",
    "        hidden_out = self.mlp(datum)\n",
    "        relu = tf.nn.relu(hidden_out)\n",
    "        if dropout:\n",
    "            relu = tf.nn.dropout(relu, keep_prob = 0.7)\n",
    "        logits = self.output_layer(relu)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_batch_encoding(datum, outputs, len_index):\n",
    "#     batch_outputs = []\n",
    "#     cur_batch_size = datum[0].shape[0]\n",
    "#     for i in range(cur_batch_size):\n",
    "#         sen_len = int(datum[len_index][i])\n",
    "#         batch_outputs.append(outputs[sen_len-1][i])\n",
    "#     return tf.convert_to_tensor(batch_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLI_loss_fun(model1, model2, datum):\n",
    "    u, s1 = model1(datum[0], datum[1], False, dummy_state)\n",
    "    v, s2 = model1(datum[2],datum[3], False, dummy_state)\n",
    "    mlp_input = tf.concat( [tf.concat([u,v],1), tf.abs(tf.subtract(u,v)), tf.multiply(u,v)], 1 )\n",
    "    logits = model2(mlp_input, True)\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=datum[4])\n",
    "    return tf.reduce_sum(loss) / tf.cast(datum[0].shape[0], dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(grads_and_vars, clip_ratio):\n",
    "  gradients, variables = zip(*grads_and_vars)\n",
    "  clipped, _ = tf.clip_by_global_norm(gradients, clip_ratio)\n",
    "  return zip(clipped, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppl(model1, model2 , dataset):\n",
    "    total_loss = 0.\n",
    "    total_words = 0\n",
    "    for batch_num, datum in enumerate(dataset):\n",
    "        avg_loss = NLI_loss_fun(model1, model2 , datum)\n",
    "        total_loss = avg_loss * tf.cast(datum[0].shape[0], dtype = tf.float32)\n",
    "        total_words += tf.cast(datum[0].shape[0], dtype = tf.float32)\n",
    "        \n",
    "#         if batch_num % 50 == 0:\n",
    "#             print(f'ppl Done batch: {batch_num}')\n",
    "    loss = total_loss / total_words\n",
    "    return np.exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_table = lookup_ops.index_table_from_file(vocab_file, default_value=0)\n",
    "NLI_label_vocab = lookup_ops.index_table_from_file(label_vocab_file, default_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NLI_create_dataset(nli_file, vocab_table, NLI_label_vocab, batch_size)\n",
    "dev_file = '../data/nli_dev_tokenized.txt'\n",
    "valid_dataset = NLI_create_dataset(dev_file, vocab_table, NLI_label_vocab, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datum = next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=0.002)\n",
    "loss_and_grads_fun = tfe.implicit_value_and_gradients(NLI_loss_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_state = tf.convert_to_tensor(np.zeros(num_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size, embedding_size, num_neurons, 'gru')\n",
    "decoder = NLI_Decoder(num_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# u1 = encoder(test_datum[0], test_datum[1])\n",
    "# u2 = encoder(test_datum[2], test_datum[3])\n",
    "# mlp_input1 = tf.concat( [tf.concat([u1,u2],1), tf.abs(tf.subtract(u1,u2)), tf.multiply(u1,u2)], 1 )\n",
    "# print(u1)\n",
    "# logits = decoder(mlp_input1, False)\n",
    "# # print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '../nli_encoder'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "root = tfe.Checkpoint(optimizer=opt, model=encoder, optimizer_step=tf.train.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start :Valid ppl: 1.2944151163101196\n",
      "Epoch: 0 Step: 50 Avg Loss: 1.0651798248291016\n",
      "INFO:tensorflow:Sun Oct 14 17:18:46 2018: Epoch: 0 Step: 50 ppl improved:  1.2729\n",
      "Epoch: 0 Step: 100 Avg Loss: 0.9480184316635132\n",
      "INFO:tensorflow:Sun Oct 14 17:22:29 2018: Epoch: 0 Step: 100 ppl improved:  1.2712\n",
      "Epoch: 0 Step: 150 Avg Loss: 0.9491152763366699\n",
      "INFO:tensorflow:Sun Oct 14 17:26:16 2018: Epoch: 0 Step: 150 ppl improved:  1.2525\n",
      "Epoch: 0 Step: 200 Avg Loss: 0.8812267780303955\n",
      "INFO:tensorflow:Sun Oct 14 17:30:16 2018: Epoch: 0 Step: 200 ppl improved:  1.2489\n",
      "Epoch: 0 Step: 250 Avg Loss: 0.8993968367576599\n",
      "INFO:tensorflow:Sun Oct 14 17:34:23 2018: Epoch: 0 Step: 250 ppl improved:  1.2482\n",
      "Epoch: 0 Step: 300 Avg Loss: 0.8707708120346069\n",
      "INFO:tensorflow:Sun Oct 14 17:38:29 2018: Epoch: 0 Step: 300 ppl improved:  1.2473\n",
      "Epoch: 0 Step: 350 Avg Loss: 0.8958591818809509\n",
      "INFO:tensorflow:Sun Oct 14 17:43:02 2018: Epoch: 0 Step: 350 ppl improved:  1.2438\n",
      "Epoch: 0 Step: 400 Avg Loss: 0.8020581007003784\n",
      "INFO:tensorflow:Sun Oct 14 17:47:31 2018: Epoch: 0 Step: 400 ppl improved:  1.2413\n",
      "Epoch: 0 Step: 450 Avg Loss: 0.8952023983001709\n",
      "INFO:tensorflow:Sun Oct 14 17:52:11 2018: Epoch: 0 Step: 450 ppl improved:  1.2256\n",
      "Epoch0 Done!\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "STATS_STEPS = 50\n",
    "EVAL_STEPS = 50\n",
    "\n",
    "valid_ppl = compute_ppl(encoder, decoder, valid_dataset)\n",
    "print(f'Start :Valid ppl: {valid_ppl}')\n",
    "\n",
    "for epoch_num in range(NUM_EPOCHS):\n",
    "    batch_loss = []\n",
    "    dataset = dataset.shuffle(buffer_size = 10000)\n",
    "    for step_num, datum in enumerate(dataset, start=1):\n",
    "        loss_value, gradients = loss_and_grads_fun(encoder, decoder, datum)\n",
    "        batch_loss.append(loss_value)\n",
    "        \n",
    "        if step_num % STATS_STEPS == 0:\n",
    "            print(f'Epoch: {epoch_num} Step: {step_num} Avg Loss: {np.average(np.asarray(loss_value))}')\n",
    "            batch_loss = []\n",
    "        \n",
    "        if step_num % EVAL_STEPS == 0:\n",
    "            ppl = compute_ppl(encoder, decoder, valid_dataset)\n",
    "            #Save model!\n",
    "            if ppl < valid_ppl:\n",
    "                log_msg(f'Epoch: {epoch_num} Step: {step_num} ppl improved: {ppl: 0.4f}')   \n",
    "                save_path = root.save(checkpoint_prefix)\n",
    "#                 print(f'Epoch: {epoch_num} Step: {step_num} ppl improved: {ppl} old: {valid_ppl} Model saved: {save_path}')\n",
    "                valid_ppl = ppl\n",
    "            else:\n",
    "                print(f'Epoch: {epoch_num} Step: {step_num} ppl worse: {ppl} old: {valid_ppl}')\n",
    "        \n",
    "        \n",
    "        \n",
    "        opt.apply_gradients(clip_gradients(gradients, 5.0), global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    print(f'Epoch{epoch_num} Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
